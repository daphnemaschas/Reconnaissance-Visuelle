{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./Figs/cs-logo.png' width=200></center>\n",
    "\n",
    "\n",
    "\n",
    "<h6><center></center></h6>\n",
    "\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>LAB 3: Harris Detector and Panorama Creation </center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n",
    "\n",
    "The objective of this lab is to develop your own interest point detector using Harris' method (refer to the course).\n",
    "\n",
    "## Exercise 1: Calculating the Harris Criterion\n",
    "Here, you are required to write a function or set of functions that return the Harris corner map $ H = det C - \\alpha (trace C)^{2}$ for a given image and a given scale (window size). Use $\\alpha = 0.04$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Calculate $I_{x}$ and $I_{y}$ gradient in $x$ and $y$ of a smoothed image using the Sobel operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(img):\n",
    "    \"\"\"\n",
    "    Computes Ix and Iy gradient in x and y of a smoothed image using the Sobel operator\n",
    "    \"\"\"\n",
    "    sobel_x = cv2.Sobel(img, cv2.CV_664F, 1, 0, ksize=3) # cv2.CV_64F: profondeur de l'image de sortie\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_664F, 1, 0, ksize=3) # k_size: taille du noyau\n",
    "    return sobel_x, sobel_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Calculate $I_{x}^{2}$, $I_{x}^{2}$ and $I_{xy}= I_{x} \\times I_{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_magnitude(img):\n",
    "    \"\"\"\n",
    "    Compute Ix^2, Iy^2 and Ix.Iy\n",
    "    \"\"\"\n",
    "    Ix, Iy = compute_gradients(img)\n",
    "    return Ix**2, Iy**2, Ix*Iy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Smooth each of the previous images with a Gaussian filter of size $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_smooth(img):\n",
    "    \"\"\"\n",
    "    Compute Gaussian Smoothing\n",
    "    \"\"\"\n",
    "    blurred_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    return blurred_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "At each pixel, calculate the Harris function:$ H = det C - \\alpha (trace C)^{2}$ with $\\alpha = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harris_function(img, alpha=0.04):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    gray = np.float32(gray)\n",
    "    dst = cv2.cornerHarris(gray, 2, 3, alpha)\n",
    "    return dst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the house image and display the $H$ image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Data/house.jpg\"\n",
    "img = cv2.imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"house.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = compute_harris_function(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[dst>0.01*dst.max()]=[0,0,255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cv2.imshow('dst', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Corner detection\n",
    "\n",
    "The aim here is to set up functions to detect corners from the corner map constructed in the previous section.\n",
    "\n",
    "Write a function to binarize the corner map using a thresholding operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to extract local maxima on a $3-$times-3 neighborhood (set to 0 in the binarized image all points whose value is not greater than that of the 8 neighbors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to display the detected points by drawing a white cross at each point on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it on the house image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Several functions are available in OpenCV for calculating points of interest. :\n",
    "+ [cornerHarris](http://docs.opencv.org/3.2.0/dc/d0d/tutorial_py_features_harris.html)\n",
    "+ [cornerSubPix](http://docs.opencv.org/3.2.0/dc/d0d/tutorial_py_features_harris.html)\n",
    "+ [goodFeaturesToTrack](http://docs.opencv.org/3.2.0/d4/d8c/tutorial_py_shi_tomasi.html) which corresponds to Shi Tomasi's approach\n",
    "+ [SIFT](http://docs.opencv.org/3.2.0/da/df5/tutorial_py_sift_intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Creating a Panorama\n",
    "\n",
    "The goal of this exercise is to create a panorama from several images, using the main concepts covered in class.\n",
    "\n",
    "The principle is quite simple and follows the approach seen in class:\n",
    "\n",
    "+ The first step is to decide which image will be your source image and which will be your destination images (i.e., the images you want to match with your source image).\n",
    "\n",
    "+ Once the source image is chosen, a technique for creating a panorama is to place this image onto a larger canvas (a larger image where unknown pixels will be set to black). The [**warpAffine**](https://docs.opencv.org/3.4.0/da/d6e/tutorial_py_geometric_transformations.html) function in OpenCV could be used for this.\n",
    "\n",
    "+ The next step involves detecting and describing a set of key points for the transformed target image and the set of destination images. You can use the [SIFT](https://docs.opencv.org/3.3.0/da/df5/tutorial_py_sift_intro.html) descriptors for this.\n",
    "\n",
    "+ Then, you need to match the descriptors from the source image with those from the destination image. You can use several [matching](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html) tools provided by the OpenCV library.\n",
    "\n",
    "+ Only the top 200 matches will be kept for further processing.\n",
    "\n",
    "+ From these matches, you need to calculate the homography that transforms the source image into the destination image. Only 4 matches are necessary to calculate this homography, but it is common to use more with the RANSAC approach explained very simply [here](http://eric-yuan.me/ransac/) and available in OpenCV (documentation [here](https://docs.opencv.org/3.4.0/d9/dab/tutorial_homography.html)) as a parameter in the **findHomography** function.\n",
    "\n",
    "+ Apply the obtained homography to the destination image.\n",
    "\n",
    "+ Merge the target image and the destination image.\n",
    "\n",
    "<center><img src='./Figs/panorama.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import useful files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: upload your images and select source and destination images.\n",
    "\n",
    "You can place your various images in the [`Data`](.\\Data) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: calculating points of interest and their descriptions\n",
    "\n",
    "For this step, you can try out several of the existing approaches available in the OpenCV library, such as :\n",
    "+ SIFT (scale-invariant feature transform): see [here](https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html)\n",
    "+ SURF (Speeded-Up Robust Features): very similar to SIFT, but with an implementation that enables rapid calculation of points and descriptors (see [here](https://docs.opencv.org/master/df/dd2/tutorial_py_surf_intro.html)).\n",
    "+ ORBFASTBRIEF (Oriented FAST and Rotated BRIEF) is a fast binary descriptor based on the combination of the FAST (Features from Accelerated Segment Test) key point detector and the BRIEF (Binary robust independent elementary features) descriptor. It is rotationally invariant and robust to noise. It was developed in the OpenCV labs and is an efficient and free alternative to SIFT. (see [here](https://docs.opencv.org/4.5.1/d1/d89/tutorial_py_orb.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Finding matching pairs\n",
    "\n",
    "The next step is to find candidate matching pairs between two images. Here too, several approaches are possible, such as [`BFMatcher.knnMatch`](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html). This approach measures the distance between each pair of keypoint descriptors and returns for each keypoint its k best matches with the minimum distance.\n",
    "\n",
    "It is then necessary to apply a ratio filter to keep only the correct matches. Indeed, to obtain a reliable match, the matched key points must be significantly closer than the closest incorrect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculating homography \n",
    "\n",
    "Once we've matched at least four pairs of key points, we can transform one image relative to the other. This operation is called image warping. Two images of the same plane surface in space are linked by a homography. Homographies are geometric transformations that have 8 free parameters and are represented by a 3x3 matrix. They represent any distortion applied to an image as a whole (as opposed to local distortions). Consequently, to obtain the transformed detected image, we need to calculate the homography matrix and apply it to the detected image.\n",
    "\n",
    "The RANSAC algorithm can be used to detect outliers and eliminate them before determining the final homography. It is directly integrated into OpenCV's [`findHomography`](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth step: image warping\n",
    "\n",
    "Apply a warp transformation using the homography matrix obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your chain to the various imahes collected. What do you recommend? What kind of improvements would you suggest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go deeper:\n",
    "+ https://www.pyimagesearch.com/2018/12/17/image-stitching-with-opencv-and-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
